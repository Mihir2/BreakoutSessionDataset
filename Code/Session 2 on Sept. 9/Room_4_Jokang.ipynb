{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Room_4_Jokang.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"I96fa6za77L7","colab_type":"text"},"source":["# List Full Names of all the participants in your team below:\n","\n","1. Michael Murphy\n","2. Xingyu Chen\n","3. Mohit Gokul Murali\n","4. Messiah Smith-Bonet\n","5. Daniel Ip\n","6. Jonathan Choi\n","7. Sagarika Suresh Thimmanayakanapalya\n","8.Trishla Chaurasia\n","9.Shuoling Li\n","10.Yinxia Chen\n","11."]},{"cell_type":"markdown","metadata":{"id":"Jm71MEiDi659","colab_type":"text"},"source":["Hello Machine Learning Engineer Jokang Team, \n","\n","You have been given a data which is obtained from **Dishonest Internet users Dataset Data Set**. In pervasive computing the interacting users are not able to obtain information about the trustworthiness of each other. Thus, unfair users can act maliciously towards others. Your task is to propose a solution to enable and evaluate the trustworthiness of each user by monitoring the behavior of each other during their interaction on the network. These behaviors are represented by the features as descibed in the attributes information.\n","\n","Number of Instances: 322 <br>\n","Number of Attributes: 5 (including the target variable `y`)\n","\n","Attribute Information: \n","  * **y:**  TS {trustworthy, untrustworthy} Trust Score. It  is the score that an entity gives to another entity at the end of each direct interaction.\n","  * **f1:** CT {CT_range_1, CT_range_2, CT_range_3, CT_range_4} CT - Counting Trust. It is used to count how many trustworthy transactions (belonging to a specific context) occur after the last untrustworthy transaction.\n","  * **f2:** CU {CU_range_1, CU_range_2, CU_range_3, CU_range_4} Counting Un-trust. It is used to count how many untrustworthy transactions (belonging to a specific context) occur after the last trustworthy transaction.\n","  * **f3:** LT {LT_range_1, LT_range_2, LT_range_3, LT_range_4} Last Time. It is used to take into account of the date at which the last experience in a specific context took place.\n","  * **f4:** TC {sport, game, ECommerce, holiday} Transactions Context. It is used to identify the type of transaction, such as game, e-commerce, social network and others.\n","\n","There are no missing Attribute Values.\n","\n","Based on the features mentioned above, your task is to build a ML based decision making system to predict the trustworthiness of each user on the network.\n"]},{"cell_type":"markdown","metadata":{"id":"7h_DIZawl5Dr","colab_type":"text"},"source":["\n","### **Q1:** Which type of Machine Learning Algorithm and which exact ML algorithm would you use for prediction and why? \n","\n","Answer: This problem can be solved with a Supervised - classification algorithm, specifically Logistic Regression."]},{"cell_type":"markdown","metadata":{"id":"qBbYTZa_l93K","colab_type":"text"},"source":["### **Q2:** In the following code cell implement the following:\n","* Step 1: Import the dataset (dishonest_internet_users.csv) using Pandas Dataframe (Step 1 Implemented already)\n","* Step 2: Map the strings in each feature column to numbers\n","* Step 3: Partition your dataset into training testing and validation using sklearns train_test_split library\n","* Step 4: Split the features and target labels into seperate variables\n","* Step 5: Scale the features using sklearns min max scaling function\n","* Step 6: If there is an off the shelf library of the ML algorithm, you can try and input the data to the library."]},{"cell_type":"code","metadata":{"id":"E8a_sw3riu9l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":262},"executionInfo":{"status":"ok","timestamp":1599752939847,"user_tz":240,"elapsed":617,"user":{"displayName":"Mihir Hemant Chauhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilYoSLpXB8B3bEittwsBFYECo7WO6QjgO2KEdR=s64","userId":"14043489589883647341"}},"outputId":"17d934ac-ab7f-4074-fe02-b5389cd3c0bd"},"source":["# Step 1 already implemented\n","import pandas as pd\n","import io\n","import requests\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.linear_model import LogisticRegression\n","url=\"https://raw.githubusercontent.com/Mihir2/BreakoutSessionDataset/master/dishonest_internet_users.csv\"\n","s = requests.get(url).content\n","data = pd.read_csv(io.StringIO(s.decode('utf-8')))\n","\n","# Step 2: Map the strings in each feature column to numbers\n","\n","data[\"y\"] = data[\"y\"].map({'trustworthy': 0, 'untrustworthy': 1})\n","data[\"f1\"] = data[\"f1\"].map({'CT_range_1' : 0, 'CT_range_2' : 1, 'CT_range_3' : 2, 'CT_range_4' : 3})\n","data[\"f2\"] = data[\"f2\"].map({'CU_range_1' : 0, 'CU_range_2' : 1, 'CU_range_3' : 2, 'CU_range_4' : 3})\n","data[\"f3\"] = data[\"f3\"].map({'LT_range_1' : 0, 'LT_range_2' : 1, 'LT_range_3' : 2, 'LT_range_4' : 3})\n","data[\"f4\"] = data[\"f4\"].map({'sport': 0, 'game': 1, 'ECommerce': 2, 'holiday' : 3})\n","print(data)\n","\n","X = data.loc[:, data.columns != 'y']\n","Y = data['y']\n","\n","# Step 3: Partition your dataset into training testing and validation using sklearns train_test_split library\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7,test_size=0.3)\n","\n","#Step 4: Split the features and target labels into seperate variables\n","features = data[['f1','f2','f3','f4']]\n","label = data[[\"y\"]]\n","\n","#Step 5: Scale the features using sklearns min max scaling function\n","scaler = MinMaxScaler()\n","X_scaler_train = scaler.fit_transform(X_train)\n","X_scaler_test = scaler.transform(X_test)\n","\n","# Step 6: If there is an off the shelf library of the ML algorithm, you can try and input the data to the library.\n","logmodel = LogisticRegression()           \n","logmodel.fit(X_scaler_train,Y_train)    \n","\n","LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["     y  f1   f2  f3  f4\n","0    0   3  0.0   3   0\n","1    0   3  0.0   3   0\n","2    1   0  3.0   3   0\n","3    1   1  0.0   3   0\n","4    1   2  0.0   3   0\n","..  ..  ..  ...  ..  ..\n","317  1   1  0.0   1   3\n","318  0   0  0.0   0   3\n","319  1   0  0.0   0   3\n","320  1   0  0.0   0   3\n","321  1   1  0.0   1   3\n","\n","[322 rows x 5 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LHMtINgzml4X","colab_type":"text"},"source":["### **Q3:**  How would you evaluate your trained model?\n","\n","Answer: We can use Cross-Entropy as the loss function. The lower the cost, the better the model.\n","\n","<font color=red>TA Answer: For evaluating the model, we can find accuracy, precision and recall for the 2-class classification model (Logistic Regression)</font>"]},{"cell_type":"code","metadata":{"id":"3YBd5bcUl2GA","colab_type":"code","colab":{}},"source":["from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","predictions = logmodel.predict(X_scaler_test)\n","print(confusion_matrix(Y_test, predictions))\n","print(classification_report(Y_test, predictions))"],"execution_count":null,"outputs":[]}]}