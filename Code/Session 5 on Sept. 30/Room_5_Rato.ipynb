{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Room_5_Rato.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"I9438QI_5xcb"},"source":["# List Full Names of all the participants in your team below:\n","1. Sagarika Suresh\n","2. Gaurav Toravane\n","3. Michael Murphy \n","4. Sindhu Pvp\n","5. Gurleen Kaur\n","6. Kevin Lin\n","7. Messiah Smith-Bonet\n","8. Nitish Dhinaharan\n","9. Vinciwu\n","10. \n","11. \n"]},{"cell_type":"markdown","metadata":{"id":"dme0LXMX5xUn"},"source":["Hello Machine Learning Engineer Rato Team, \n","\n","You have been given a data which is obtained from **Facebook Comment Volume Dataset**.\n","\n","Number of Instances: 40949 <br>\n","Number of Attributes: 38 (including the target variable `y`)\n","\n","Attribute Information: \n","\n","* **y**\n","Target Variable\n","Decimal\n","Target\n","The no of comments in next H hrs(H is given in Feature no 39).\n","* **f1**\n","Page Popularity/likes\n","Decimal Encoding\n","Page feature\n","Defines the popularity or support for the source of the document.\n","* **f2**\n","Page Checkings\n","Decimal Encoding\n","Page feature\n","Describes how many individuals so far visited this place. This feature is only associated with the places eg:some institution, place, theater etc.\n","* **f3**\n","Page talking about\n","Decimal Encoding\n","Page feature\n","Defines the daily interest of individuals towards source of the document/ Post. The people who actually come back to the page, after liking the page. This include activities such as comments, likes to a post, shares, etc by visitors to the page.\n","* **f4**\n","Page Category\n","Value Encoding\n","Page feature\n","Defines the category of the source of the document eg: place, institution, brand etc.\n","* **f5 - f29**\n","Derived\n","Decimal Encoding\n","Derived feature\n","These features are aggregated by page, by calculating min, max, average, median and standard deviation of essential features.\n","* **f30**\n","CC1\n","Decimal Encoding\n","Essential feature\n","The total number of comments before selected base date/time.\n","* **f31**\n","CC2\n","Decimal Encoding\n","Essential feature\n","The number of comments in last 24 hours, relative to base date/time.\n","* **f32**\n","CC3\n","Decimal Encoding\n","Essential feature\n","The number of comments in last 48 to last 24 hours relative to base date/time.\n","* **f33**\n","CC4\n","Decimal Encoding\n","Essential feature\n","The number of comments in the first 24 hours after the publication of post but before base date/time.\n","* **f34**\n","CC5\n","Decimal Encoding\n","Essential feature\n","The difference between CC2 and CC3.\n","* **f35**\n","Base time\n","Decimal(0-71) Encoding\n","Other feature\n","Selected time in order to simulate the scenario.\n","* **f36**\n","Post length\n","Decimal Encoding\n","Other feature\n","Character count in the post.\n","* **f37**\n","Post Share Count\n","Decimal Encoding\n","Other feature\n","This features counts the no of shares of the post, that how many peoples had shared this post on to their timeline.\n","* **f38**\n","H Local\n","Decimal(0-23) Encoding\n","Other feature\n","This describes the H hrs, for which we have the target variable/ comments received.\n","\n","There are no missing Attribute Values.\n","\n","Your task is to implement a **Linear Regression model using Gradient Descent Solution** to predict the no. of comments in next H hrs for a post.\n","\n","## Gradient Descent Solution\n","The **genesis equation** for Linear Regression is of the form:\n","\n","$y(x,w) = W.x + b$  \n","\n","* $y(x,w)$ is predicted output,\n","* $x$ is the Input\n","* $W = [W_{1}, W_{2}, .. , W_{F}]$ are the parameters to be learned from training samples with $F$ Features\n","* $b$ is the bias\n","\n","\n","\n","YOU NEED TO IMPLEMENT an iterative solution to solve for $W$ (Gradient Descent Solution) \n","\n","<font color=\"red\">DO NOT USE SKLEARNS LINEAR REGRESSION LIBRARY DIRECTLY.</font>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GlPiya6O5xMC"},"source":["### **Question:** In the following code cell implement the following:\n","* Step 1: Import the dataset (facebook_comment.csv) using Pandas Dataframe (Step 1 Implemented already)\n","* Step 2: Partition your dataset into training testing and validation using sklearns train_test_split library and split the features and target labels into seperate variables (Step 2 Implemented already)\n","* Step 3: Scale the features using sklearns min max scaling function (Step 3 Implemented already)\n","* Step 4: Convert Scaled Features and Labels into numpy arrays with dimensions required by closed form solution (Step 4 Implemented already)\n","* Step 5: Initialize the learning rate, number of epochs, weight vector, bias scalar and other variables required for tracking cost.\n","* Step 6: Train with Training Dataset using Gradient Descent Solution\n","  Iteratively update the weights and biases for each epoch using:\n","  * Step 6.1: Use genesis equation $\\hat{y} = W^{T}.X + b$ where $W$ is the weight array, $X$ is the input features and $\\hat{y}$ is the predicted value. (You will have to perform same operation on validation set as well)\n","  * Step 6.2: Find Mean Squared Error (MSE) Loss Function (L) for training and validation set using predicted value $\\hat{y}$ and truth value $y$\n","    * MSE Train Cost = $\\frac{1}{n} \\sum_{i=1}^{i=n} (y\\_train_{i} - \\hat{y}\\_train_{i})^{2}$\n","  * Step 6.3: Find $ \\Delta W_{j} = \\frac{\\delta L}{\\delta W_{j}}$ and $ \\Delta b = \\frac{\\delta L}{\\delta b}$ where $j = 1$ to $F$(Proof for finding  $\\Delta W$ and $\\Delta b$ is available in the appendix below)\n","    * $ \\Delta W_{j} = \\frac{\\delta L}{\\delta W_{j}} = \\frac{2}{n}\\sum_{i=1}^{i=n}(y\\_train_{i} - \\hat{y}\\_train_{i})*x_{ij}$\n","    * $ \\Delta b = \\frac{\\delta L}{\\delta b} = \\frac{2}{n}\\sum_{i=1}^{i=n}(y\\_train_{i} - \\hat{y}\\_train_{i})$\n","  * Step 6.4: Update $W$ and $b$ using learning rate($\\eta$) as follows:\n","    - $W_{j} = W_{j} - \\eta*\\Delta W_{j}$\n","    - $b = b - \\eta*\\Delta b$\n","  * Step 6.5: Store MSE Cost for training and validation in cost tracking lists\n","* Step 7: Plot validation and training cost vs number of epochs (Already Implemented)\n","* Step 8: Test using Testing Dataset\n","  * Step 8.1: Use genesis equation $\\hat{y} = W^{T}.X_{test} + b$ where $W$ is the weight array, $X_{test}$ is the input test features and $\\hat{y}$ is the predicted value.\n","  * Step 8.2: Calculate Mean Squared Error (MSE) for Test Dataset\n","    * MSE Test Cost$ = \\frac{1}{n}\\sum_{i=1}^{i=n} (y\\_test_{i} - \\hat{y}\\_test_{i})^{2}$ "]},{"cell_type":"markdown","metadata":{"id":"WDludci3VAQq"},"source":["## TA Response"]},{"cell_type":"code","metadata":{"id":"cQ3FeTcfU_ug","executionInfo":{"status":"ok","timestamp":1601910890686,"user_tz":240,"elapsed":1427,"user":{"displayName":"Mihir Hemant Chauhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilYoSLpXB8B3bEittwsBFYECo7WO6QjgO2KEdR=s64","userId":"14043489589883647341"}}},"source":["# Step 1 already implemented\n","import pandas as pd\n","import io\n","import requests\n","url=\"https://raw.githubusercontent.com/Mihir2/BreakoutSessionDataset/master/AirQualitySeattle.csv\"\n","s = requests.get(url).content\n","data = pd.read_csv(io.StringIO(s.decode('utf-8')))\n","data\n","\n","# Step 2 already implemented\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","output = data['y']\n","input = data.to_numpy()[:,1:]\n","x_train, x_val_test, y_train, y_val_test = train_test_split(input, output, test_size = 0.8)\n","x_val, x_test, y_val, y_test = train_test_split(input, output, test_size = 0.5)\n","\n","# Step 3 already implemented\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","sc_xtrain = scaler.fit_transform(x_train)\n","sc_xval = scaler.fit_transform(x_val)\n","sc_xtest = scaler.transform(x_test)\n","\n","# Step 4 already implemented\n","y_train_arr = y_train.to_numpy().reshape(y_train.shape[0],1).T\n","x_train_arr = sc_xtrain.T\n","y_val_arr = y_val.to_numpy().reshape(y_val.shape[0],1).T\n","x_val_arr = sc_xval.T\n","y_test_arr  = y_test.to_numpy().reshape(y_test.shape[0],1).T\n","x_test_arr  = sc_xtest.T\n","\n","# Step 5 already implemented\n","learningrate = 0.005\n","epochs = 1000\n","bias = 0\n","\n","number_of_features         = x_train_arr.shape[0]\n","number_of_train_datapoints = x_train_arr.shape[1]\n","number_of_val_datapoints   = x_val_arr.shape[1]\n","number_of_test_datapoints  = x_test_arr.shape[1]\n","\n","weights = np.random.randn(number_of_features,1)\n","\n","training_cost_track = []\n","val_cost_track = []\n","\n","# Step 6\n","for epoch in range(epochs):\n","    \n","    # Step 6.1 y_pred = wT.X + b (For Training and Validation dataset)\n","    train_pred = np.dot(weights.T, x_train_arr) + bias\n","    val_pred   = np.dot(weights.T, x_val_arr)   + bias\n","    \n","    # Step 6.2 MSE Cost for Training and Validation Dataset\n","    train_cost = np.sum((train_pred - y_train_arr)**2)/number_of_train_datapoints\n","    val_cost   = np.sum((val_pred - y_val_arr)**2)/number_of_val_datapoints\n","    \n","    # Step 6.3: Calculate derivatives\n","    dz = train_pred - y_train_arr\n","    dw = (1/number_of_train_datapoints) * np.dot(x_train_arr, dz.T)\n","    db = (1/number_of_train_datapoints) * np.sum(dz)\n","    \n","    # Step 6.4: update weights and bias\n","    weights = weights - learningrate * dw\n","    bias = bias - learningrate * db\n","\n","    # Step 6.5: Store MSE Cost for training and validation in seperate cost tracking list\n","    training_cost_track.append(train_cost)\n","    val_cost_track.append(val_cost)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"dwv_09qBVADl","executionInfo":{"status":"ok","timestamp":1601910891716,"user_tz":240,"elapsed":397,"user":{"displayName":"Mihir Hemant Chauhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilYoSLpXB8B3bEittwsBFYECo7WO6QjgO2KEdR=s64","userId":"14043489589883647341"}}},"source":["# Step 8.1: Test using Testing Dataset: Get the predicted values \n","test_pred = np.dot(weights.T, x_test_arr) + bias\n","\n","# Step 8.2: Calculate the MSE for Testing dataset\n","test_cost = np.sum((test_pred - y_test_arr)**2)/number_of_test_datapoints"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDryzi0WU_jH"},"source":["## Student Response"]},{"cell_type":"code","metadata":{"id":"09bHp8cS4B_K","executionInfo":{"elapsed":3478,"status":"ok","timestamp":1601498980933,"user":{"displayName":"Mihir Hemant Chauhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilYoSLpXB8B3bEittwsBFYECo7WO6QjgO2KEdR=s64","userId":"14043489589883647341"},"user_tz":240},"outputId":"7397093d-24d3-4e36-c444-11953d82fdbb"},"source":["# Step 1 already implemented\n","import pandas as pd\n","import math as mt\n","import io\n","import requests\n","url=\"https://raw.githubusercontent.com/Mihir2/BreakoutSessionDataset/master/facebook_comment.csv\"\n","s = requests.get(url).content\n","data = pd.read_csv(io.StringIO(s.decode('utf-8')))\n","\n","\n","# Step 2 already implemented\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","output = data['y']\n","input = data.to_numpy()[:,1:]\n","x_train, x_val_test, y_train, y_val_test = train_test_split(input, output, test_size = 0.8)\n","x_val, x_test, y_val, y_test = train_test_split(input, output, test_size = 0.5)\n","\n","# Step 3 already implemented\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","sc_xtrain = scaler.fit_transform(x_train)\n","sc_xval = scaler.fit_transform(x_val)\n","sc_xtest = scaler.transform(x_test)\n","\n","# Step 4 already implemented\n","y_train_arr = y_train.to_numpy().reshape(y_train.shape[0],1).T\n","x_train_arr = sc_xtrain.T\n","y_val_arr = y_val.to_numpy().reshape(y_val.shape[0],1).T\n","x_val_arr = sc_xval.T\n","y_test_arr  = y_test.to_numpy().reshape(y_test.shape[0],1).T\n","x_test_arr  = sc_xtest.T\n","\n","# Step 5 already implemented\n","learningrate = 0.005\n","epochs = 1000\n","bias = 0\n","\n","number_of_features         = x_train_arr.shape[0]\n","number_of_train_datapoints = x_train_arr.shape[1]\n","number_of_val_datapoints   = x_val_arr.shape[1]\n","number_of_test_datapoints  = x_test_arr.shape[1]\n","\n","weights = np.random.randn(number_of_features,1)\n","\n","training_cost_track = []\n","val_cost_track = []\n","\n","# Step 6\n","for epoch in range(epochs):\n","    \n","    # Step 6.1 y_pred = wT.X + b (For Training and Validation dataset)\n","\n","    y_pred=np.dot(np.transpose(weights),x_train_arr) +bias\n","    y_pred_val=np.dot(np.transpose(weights),x_val_arr) +bias \n","    # Step 6.2 MSE Cost for Training and Validation Dataset   1𝑛∑𝑖=𝑛𝑖=1(𝑦_𝑡𝑟𝑎𝑖𝑛𝑖−𝑦̂ _𝑡𝑟𝑎𝑖𝑛𝑖)2\n","    mse=(pow(y_train_arr-y_pred,2)).mean()\n","    mse_val=(pow(y_val_arr-y_pred_val,2)).mean()\n","    # Step 6.3: Calculate derivatives\n","    for j in range(number_of_features):\n","        derivative_weight= (2*(y_train_arr-y_pred)*x_train_arr).mean()\n","        derivative_bias=(2*(y_train_arr-y_pred)).mean()\n","    \n","    # Step 6.4: update weights and bias\n","    weights=weights-(learningrate*derivative_weight)\n","    bias=bias-(learningrate*derivative_bias)\n","    # Step 6.5: Store MSE Cost for training and validation in seperate cost tracking list\n","    training_cost_track.append(mse)\n","    val_cost_track.append(mse_val)\n","    \n","# Step 7: Plot MSE cost for training and validation set vs number of epochs (Already Implemented)\n","import matplotlib.pyplot as plt\n","plt.title('Training and Validation Loss With Learning Rate: ' + str(learningrate))\n","plt.plot(training_cost_track, color='red', label='Training Data')\n","plt.plot(val_cost_track, color='blue', label='Validation Data')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()   \n","\n","# Step 8.1: Test using Testing Dataset: Get the predicted values \n","y_pred_test=np.dot(np.transpose(weights),x_test_arr)+bias \n","# Step 8.2: Calculate the MSE for Testing dataset\\\n","mse_test=np.power(y_test_arr - y_pred_test,2).mean()\n","mse_test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(38, 1)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"6NnliVYlS0HU"},"source":["### TESTING ####\n","\n","# Step 1 already implemented\n","import pandas as pd\n","import math as mt\n","import io\n","import requests\n","url=\"https://raw.githubusercontent.com/Mihir2/BreakoutSessionDataset/master/facebook_comment.csv\"\n","s = requests.get(url).content\n","data = pd.read_csv(io.StringIO(s.decode('utf-8')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iukgDnIPxz-j"},"source":["# Step 2 already implemented\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","output = data['y']\n","input = data.to_numpy()[:,1:]\n","x_train, x_val_test, y_train, y_val_test = train_test_split(input, output, test_size = 0.8)\n","x_val, x_test, y_val, y_test = train_test_split(input, output, test_size = 0.5)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iZo-UFijS0Hd"},"source":["# Step 3 already implemented\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","sc_xtrain = scaler.fit_transform(x_train)\n","sc_xval = scaler.fit_transform(x_val)\n","sc_xtest = scaler.transform(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBPdiTxCx0NF"},"source":["# Step 4 already implemented\n","y_train_arr = y_train.to_numpy().reshape(y_train.shape[0],1).T\n","x_train_arr = sc_xtrain.T\n","y_val_arr = y_val.to_numpy().reshape(y_val.shape[0],1).T\n","x_val_arr = sc_xval.T\n","y_test_arr  = y_test.to_numpy().reshape(y_test.shape[0],1).T\n","x_test_arr  = sc_xtest.T\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jObCmMMyS0Hm"},"source":["# Step 5 already implemented\n","learningrate = 0.005\n","epochs = 1000\n","bias = 0\n","\n","number_of_features         = x_train_arr.shape[0]\n","number_of_train_datapoints = x_train_arr.shape[1]\n","number_of_val_datapoints   = x_val_arr.shape[1]\n","number_of_test_datapoints  = x_test_arr.shape[1]\n","\n","weights = np.random.randn(number_of_features,1)\n","\n","training_cost_track = []\n","val_cost_track = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGd20XM4S0Hr"},"source":["# Step 6\n","for epoch in range(epochs):\n","    \n","    # Step 6.1 y_pred = wT.X + b (For Training and Validation dataset)\n","\n","    y_pred=np.dot(np.transpose(weights),x_train_arr) +bias\n","    y_pred_val=np.dot(np.transpose(weights),x_val_arr) +bias \n","    # Step 6.2 MSE Cost for Training and Validation Dataset   1𝑛∑𝑖=𝑛𝑖=1(𝑦_𝑡𝑟𝑎𝑖𝑛𝑖−𝑦̂ _𝑡𝑟𝑎𝑖𝑛𝑖)2\n","    mse=(pow(y_train_arr-y_pred,2)).mean()\n","    mse_val=(pow(y_val_arr-y_pred_val,2)).mean()\n","    # Step 6.3: Calculate derivatives\n","    for j in range(number_of_features):\n","        derivative_weight= (2*(y_train_arr-y_pred)*x_train_arr).mean()\n","        derivative_bias=(2*(y_train_arr-y_pred)).mean()\n","    \n","    # Step 6.4: update weights and bias\n","    weights=weights-(learningrate*derivative_weight)\n","    bias=bias-(learningrate*derivative_bias)\n","    # Step 6.5: Store MSE Cost for training and validation in seperate cost tracking list\n","    training_cost_track.append(mse)\n","    val_cost_track.append(mse_val)    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UyyzATjLS0Hv","outputId":"3d66d7d7-1915-4848-8552-94c83b6a73a3"},"source":["# Step 7: Plot MSE cost for training and validation set vs number of epochs (Already Implemented)\n","import matplotlib.pyplot as plt\n","plt.title('Training and Validation Loss With Learning Rate: ' + str(learningrate))\n","plt.plot(training_cost_track, color='red', label='Training Data')\n","plt.plot(val_cost_track, color='blue', label='Validation Data')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()  "],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"EC274SviS0H0","outputId":"7bdc5ff7-61cf-4689-ed70-bc7d607093a4"},"source":["# Step 8.1: Test using Testing Dataset: Get the predicted values \n","\n","y_pred_test=np.dot(np.transpose(weights),x_test_arr)+bias \n","# Step 8.2: Calculate the MSE for Testing dataset\n","mse_test=np.power(y_test_arr - y_pred_test,2).mean()\n","mse_test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.7377367674369274e+31"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"KU3c2UpFx5NL"},"source":["### Appendix (Proof)\n","\n","![!picture](https://drive.google.com/uc?export=view&id=1MVcYLfEzAlKcumhLiwrsmGSn0-AAK3tg)"]}]}