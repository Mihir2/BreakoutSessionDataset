{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Room_4_Jokang.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6zleMtZ196Fz"},"source":["# List Full Names of all the participants in your team below:\n","1. Gurleen kaur\n","2. Xianxin lin\n","3. Joshua bukaty\n","4. Kevin Lin\n","5. Surya Muthiah Pillai\n","6. Joseph Mccart\n","7. Yingwei Li\n","8. \n","9. \n","10. \n","11. "]},{"cell_type":"markdown","metadata":{"id":"vkQkNyTy-v07"},"source":["Hello Machine Learning Engineer Jokang Team, \n","\n","You have been given a data which is obtained from **Breast Cancer Dataset**. The dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\n","\n","Number of Instances: 306 <br>\n","Number of Attributes: 3 (including the target variable `y`)\n","\n","Attribute Information: \n","  * **y**: Survival status (class attribute) \n","      * -1 = the patient died within 5 year\n","      * 1 = the patient survived 5 years or longer\n","  * **f1**: Age of patient at time of operation (numerical)\n","  * **f2**: Patient's year of operation (year - 1900, numerical)\n","  * **f3**: Number of positive axillary nodes detected (numerical)\n","\n","There are no missing Attribute Values.\n","\n","Your task is to implement a **Logistic Regression using Variational Bayes EM approach** to predict if the patient survived within 5 years of contracting breast cancer.\n"]},{"cell_type":"markdown","metadata":{"id":"dWr5jiBl-9x4"},"source":["\n","## Variational Bayesian Logistic Regression\n","\n","Similar to the Laplace Method for Bayesian Logistic Regression, the Variational Methods focuses on the use of Gaussian Approximation to the posterior distribution. In fact the variational approcimation to the posterior distribution leads to improved accuracy compared to the laplace method. The variational approach is optimizing a well defined objective function given by a rigorous bound on the model evidence. \n","\n","\n","\n","<font color=\"Green\">USE THE VBLogisticRegression CLASS DIRECTLY.</font>"]},{"cell_type":"markdown","metadata":{"id":"PKK499bw-2IT"},"source":["### **Question 1:** In the following code cell implement Step 5, 6 and 7. \n","### **Question 2:** MAP the formulaes to the code snippets of VBLR class in Step 5.1, Step 5.2 and Step 6.1 and Step 6.2 : \n","* Step 1: Import the dataset using Pandas Dataframe (Step 1 Implemented already)\n","* Step 2: Partition your dataset into training and testing using sklearns train_test_split library and split the features and target labels into seperate variables (Step 2 Implemented already)\n","* Step 3: Scale the training and testing features using sklearns min max scaling function (Step 3 Implemented already)\n","* Step 4: Convert Labels into numpy arrays (Step 4 Implemented already)\n","* Step 5: Train with Training Dataset using VBLR Solution. Below are the steps required for training VBLR model:\n","  * Step 5.1: Initialize Variables for Training (eps, bias (w0), parameters of approximate distribution (a,b)) \n","  * Step 5.2: Run EM algorithm Iteratively to update approximate variational posterior distribution q(w, alpha): \n","    * Step 5.2.1: E-Step: Update approximation of posterior distribution q(w, alpha) = q(w)*q(alpha) \n","      * Update q(w) \n","        * Find lambda(eps) using $\\lambda (\\xi) = -\\frac{1}{2\\xi}[\\sigma(\\xi) - \\frac{1}{2}] = -\\frac{1}{4}\\frac{1 - e^{-\\xi}}{\\xi(1 + e^{-\\xi})}$\n","        * Update mean and variance for approximate posterior q(w) using <br>\n","        $\\Sigma_{N}^{-1} = E[\\alpha]I + 2\\Sigma_{n=1}^{N}\\lambda(\\xi_{n}X_{n}X_{n}^{T})$ <br>\n","        $\\Sigma_{N}^{-1}\\mu_{N} = \\Sigma_{n=1}^{N} (t_{n} - \\frac{1}{2})X$ <br>\n","        $E[\\alpha] = \\frac{a_{N}}{b_{N}}$\n","      * Update q(alpha) (a is constant, b needs to updated) <br>\n","        $a_{N} = a_{0} + \\frac{M}{2}$ where M: Number of Features (Fixed)<br>\n","        $b_{N} = b_{0} + \\frac{1}{2}E[w^{T}w]$ <br>\n","        $E[w^{T}w] = \\Sigma_{N} + \\mu_{N}^{T}\\mu_{N}$ <br>\n","    * Step 5.2.2: M-Step: Update Parameters eps which controls accuracy of local variational approximation to lower bound $(\\xi_{n}^{new})^{2} = X_{n}^{T}(\\Sigma_{N} + \\mu_{N}\\mu_{N}^{T})X_{n}$\n","* Step 6: Test using Testing Dataset\n","  * Step 6.1: Use probit function equation as genesis equation:\n","    * $\\sigma(\\frac{\\mu.X + bias}{\\sqrt{1 + \\pi\\Sigma^{2}/8}})$ \n","  * Step 6.2: Calculate Accuracy using Sklearns.Metrics library."]},{"cell_type":"code","metadata":{"id":"J2rF2xa1XNqb","executionInfo":{"status":"ok","timestamp":1603322420603,"user_tz":-330,"elapsed":2063,"user":{"displayName":"Gurleen Kaur","photoUrl":"","userId":"09299882833551484325"}},"outputId":"de9a64e8-c9e2-4c79-f334-20d24f330c7d","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from scipy.special import expit, exprel\n","from scipy.linalg import solve_triangular\n","\n","class BayesianLogisticRegression():\n","\n","    def __init__(self, n_iter, tol):\n","        self.n_iter = n_iter\n","        self.tol = tol\n","\n","    def fit(self, X, y):\n","        '''\n","        Fits Bayesian Logistic Regression\n","        Parameters\n","        -----------\n","        X: array-like of size (n_samples, n_features)\n","           Training data, matrix of explanatory variables\n","\n","        y: array-like of size (n_samples, )\n","           Target values\n","\n","        Returns\n","        -------\n","        self: object\n","           self\n","        '''\n","\n","        # Get the number of classes\n","        self.classes_ = np.unique(y)\n","        n_classes = len(self.classes_)\n","\n","        # Augment X with ones to include bias\n","        X = self._add_intercept(X)\n","\n","        self.coef_, self.sigma_, self.intercept_ = [0], [0], [0]\n","\n","        # make classifier for just binary classification\n","        coef_, sigma_ = self._fit(X, y) ## Calling VBLR Fit\n","        self.intercept_[0], self.coef_[0] = self._get_intercept(coef_)\n","        self.sigma_[0] = sigma_\n","        self.coef_ = np.asarray(self.coef_)\n","        return self\n","\n","    def predict_prob(self, X):\n","        print(\"Testing Logistic Regression Model (Calculating Class Probabilities) . .\")\n","        '''\n","        Predicts probabilities of targets for test set\n","\n","        Parameters\n","        ----------\n","        X: array-like of size [n_samples_test,n_features]\n","           Matrix of explanatory variables (test set)\n","\n","        Returns\n","        -------\n","        probs: numpy array of size [n_samples_test]\n","           Estimated probabilities of target classes\n","        '''\n","        # construct separating hyperplane\n","        scores = (np.dot(X,self.coef_.T) + self.intercept_).flatten()\n","        X = self._add_intercept(X)\n","\n","        #step 6.1\n","        # probit approximation to predictive distribution\n","        sigma = self._get_sigma(X)\n","        ks = 1. / (1. + np.pi * sigma / 8) ** 0.5\n","        probs = expit(scores.T * ks).T\n","\n","        return probs\n","\n","# ============== VB Logistic Regression (with Jaakola Jordan bound) ==================s\n","\n","\n","def lam(eps):\n","    ''' Calculates lambda eps (used for Jaakola & Jordan local bound) '''\n","    eps = -abs(eps)\n","    return 0.25 * exprel(eps) / (np.exp(eps) + 1)\n","\n","class VBLogisticRegression(BayesianLogisticRegression):\n","    '''\n","    Variational Bayesian Logistic Regression with local variational approximation.\n","\n","\n","    Parameters:\n","    -----------\n","    n_iter: int, optional (DEFAULT = 50 )\n","       Maximum number of iterations\n","\n","    tol: float, optional (DEFAULT = 1e-3)\n","       Convergence threshold, if cange in coefficients is less than threshold\n","       algorithm is terminated\n","\n","    fit_intercept: bool, optinal ( DEFAULT = True )\n","       If True uses bias term in model fitting\n","\n","    a: float, optional (DEFAULT = 1e-6)\n","       Rate parameter for Gamma prior on precision parameter of coefficients\n","\n","    b: float, optional (DEFAULT = 1e-6)\n","       Shape parameter for Gamma prior on precision parameter of coefficients\n","\n","\n","    Attributes\n","    ----------\n","    coef_ : array, shape = (n_features)\n","        Coefficients of the regression model (mean of posterior distribution)\n","    sigma_ : array, shape = (n_features, n_features)\n","        estimated covariance matrix of the weights, computed only\n","        for non-zero coefficients\n","    intercept_: array, shape = (n_features)\n","        intercepts\n","\n","    References:\n","    -----------\n","   [1] Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )\n","   [2] Murphy 2012, Machine Learning A Probabilistic Perspective ( Chapter 21 )\n","    '''\n","\n","    def __init__(self, n_iter=50, tol=1e-3,\n","                 a=1e-4, b=1e-4):\n","        super(VBLogisticRegression, self).__init__(n_iter, tol)\n","        self.a = a\n","        self.b = b\n","        self._mask_val = 0.\n","\n","    def _fit(self, X, y):\n","        '''\n","        Fits single classifier for each class (for OVR framework)\n","        '''\n","        print(\"Training Logistic Regression Model with Variational Bayes EM approach . .\")\n","        eps = 1\n","        n_samples, n_features = X.shape\n","        XY = np.dot(X.T, (y - 0.5))\n","        w0 = np.zeros(n_features)\n","\n","        # hyperparameters of q(alpha) (approximate distribution of precision\n","        # parameter of weights)\n","        a = self.a + 0.5 * n_features\n","        b = self.b\n","\n","        \n","        for i in range(self.n_iter):\n","            #step 5.2.1\n","            # In the E-step we update approximation of\n","            # posterior distribution q(w,alpha) = q(w)*q(alpha)\n","            # --------- update q(w) ------------------\n","            l = lam(eps)\n","            w, Ri = self._posterior_dist(X, l, a, b, XY)\n","            # -------- update q(alpha) ---------------\n","            b = self.b + 0.5 * (np.sum(w[1:] ** 2) + np.sum(Ri[1:, :] ** 2))\n","\n","            #step 5.2.2\n","            # -------- update eps  ------------\n","            # In the M-step we update parameter eps which controls\n","            # accuracy of local variational approximation to lower bound\n","            XMX = np.dot(X, w) ** 2\n","            XSX = np.sum(np.dot(X, Ri.T) ** 2, axis=1)\n","            eps = np.sqrt(XMX + XSX)\n","\n","            # convergence\n","            if np.sum(abs(w - w0) > self.tol) == 0 or i == self.n_iter - 1:\n","                break\n","            w0 = w\n","\n","        l = lam(eps)\n","        coef_, sigma_ = self._posterior_dist(X, l, a, b, XY, True)\n","        return coef_, sigma_\n","\n","    def _add_intercept(self, X):\n","        '''Adds intercept to data matrix'''\n","        return np.hstack((np.ones([X.shape[0], 1]), X))\n","\n","    def _get_intercept(self, coef):\n","        ''' Returns intercept and coefficients '''\n","        return coef[0], coef[1:]\n","\n","    def _get_sigma(self, X):\n","        ''' Compute variance of predictive distribution'''\n","        return np.asarray([np.sum(np.dot(X, s) * X, axis=1) for s in self.sigma_])\n","\n","    def _posterior_dist(self, X, l, a, b, XY, full_covar=False):\n","        '''\n","        Finds gaussian approximation to posterior of coefficients using\n","        local variational approximation of Jaakola & Jordan\n","        '''\n","        sigma_inv = 2 * np.dot(X.T * l, X)\n","        alpha_vec = np.ones(X.shape[1]) * float(a) / b\n","        alpha_vec[0] = np.finfo(np.float16).eps # Bias variable\n","        np.fill_diagonal(sigma_inv, np.diag(sigma_inv) + alpha_vec) # sigma inv\n","        R = np.linalg.cholesky(sigma_inv)\n","        Z = solve_triangular(R, XY, lower=True)\n","        mean = solve_triangular(R.T, Z, lower=False)\n","\n","        Ri = solve_triangular(R, np.eye(X.shape[1]), lower=True)\n","        if full_covar:\n","            sigma = np.dot(Ri.T, Ri)\n","            return mean, sigma\n","        else:\n","            return mean, Ri\n","\n","# Step 1 already implemented\n","import pandas as pd\n","import io\n","import requests\n","\n","url = \"https://raw.githubusercontent.com/Mihir2/BreakoutSessionDataset/master/haberman_lr.csv\"\n","s = requests.get(url).content\n","data = pd.read_csv(io.StringIO(s.decode('utf-8')))\n","\n","# Step 2 already implemented\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","\n","output = data['y']\n","input = data.to_numpy()[:, 1:]\n","x_train, x_test, y_train, y_test = train_test_split(input, output, test_size=0.2)\n","\n","# Step 3 already implemented\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","x_train_arr = scaler.fit_transform(x_train)\n","x_test_arr = scaler.transform(x_test)\n","\n","# Step 5 already implemented\n","y_train_arr = y_train.to_numpy()\n","y_test_arr = y_test.to_numpy()\n","\n","\n","# Step 6 Train using Variational Bayesian Regression Class (Calculate model parameters)\n","bayLogRes = VBLogisticRegression()\n","bayLogRes.fit(x_train_arr,y_train_arr)\n","\n","\n","# Step 7 Test using Variational Bayesian Regression Class (Calculate class Probabilities)\n","\n","y_pred = bayLogRes.predict_prob(x_test_arr)\n","y_pred =[1 if pred>=0.5 else 0 for pred in y_pred.ravel()]\n","\n","\n","# Step 8 Calculate Accuracy using Sklearns library\n","#step 6.2\n","from sklearn.metrics import accuracy_score\n","accuracy_test=accuracy_score(y_test_arr.ravel(),y_pred)\n","print(accuracy_test)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Logistic Regression Model with Variational Bayes EM approach . .\n","Testing Logistic Regression Model (Calculating Class Probabilities) . .\n","0.6612903225806451\n"],"name":"stdout"}]}]}