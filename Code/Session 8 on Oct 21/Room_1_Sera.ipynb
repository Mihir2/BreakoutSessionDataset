{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Room_1_Sera.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jIuMbjKY9z8v"},"source":["# List Full Names of all the participants in your team below:\n","1. Daniel Walsh\n","2. Faizaan Arshad\n","3. Jonathan Choi\n","4. Mayank Lara\n","5. Mohit Gokul Murali\n","6. Rishabh Kumar\n","7. Sagarika Suresh T\n","8. Shuoling Li\n","9. Tulika Sharma\n","10. \n","11. "]},{"cell_type":"markdown","metadata":{"id":"x12OrY6v9_jr"},"source":["Hello Machine Learning Engineer Sera Team, \n","\n","You have been given a **Banknote Authentication Dataset** from UCI data repository. The data features were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features (f1,f2,f3) from images.\n","\n","Number of Instances: 1372 <br>\n","Number of Attributes: 5 (including the target variable `y`)\n","\n","Attribute Information: \n","   * **y:**  Forged or not (0 Forged or +1 Not Forged)\n","   * **f1:** variance of Wavelet Transformed image (continuous)\n","   * **f2:** skewness of Wavelet Transformed image (continuous)\n","   * **f3:** curtosis of Wavelet Transformed image (continuous)\n","   * **f4:** entropy of image (continuous)\n","\n","There are no missing Attribute Values.\n","\n","Your task is to implement a **Logistic Regression using Variational Bayes EM appraoch** for predicting given a banknote is forged or not.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uEdfLr5d-C9X"},"source":["\n","## Variational Bayesian Logistic Regression\n","\n","Similar to the Laplace Method for Bayesian Logistic Regression, the Variational Methods focuses on the use of Gaussian Approximation to the posterior distribution. In fact the variational approcimation to the posterior distribution leads to improved accuracy compared to the laplace method. The variational approach is optimizing a well defined objective function given by a rigorous bound on the model evidence. \n","\n","\n","\n","<font color=\"Green\">USE THE VBLogisticRegression CLASS DIRECTLY.</font>"]},{"cell_type":"markdown","metadata":{"id":"dSb3y_gm-GfW"},"source":["### **Question 1:** In the following code cell implement Step 5, 6 and 7. \n","### **Question 2:** MAP the formulaes to the code snippets of VBLR class in Step 5.1, Step 5.2 and Step 6.1 and Step 6.2 : \n","* Step 1: Import the dataset using Pandas Dataframe (Step 1 Implemented already)\n","* Step 2: Partition your dataset into training and testing using sklearns train_test_split library and split the features and target labels into seperate variables (Step 2 Implemented already)\n","* Step 3: Scale the training and testing features using sklearns min max scaling function (Step 3 Implemented already)\n","* Step 4: Convert Labels into numpy arrays (Step 4 Implemented already)\n","* Step 5: Train with Training Dataset using VBLR Solution. Below are the steps required for training VBLR model:\n","  * Step 5.1: Initialize Variables for Training (eps, bias (w0), parameters of approximate distribution (a,b)) \n","  * Step 5.2: Run EM algorithm Iteratively to update approximate variational posterior distribution q(w, alpha): \n","    * Step 5.2.1: E-Step: Update approximation of posterior distribution q(w, alpha) = q(w)*q(alpha) \n","      * Update q(w) \n","        * Find lambda(eps) using $\\lambda (\\xi) = -\\frac{1}{2\\xi}[\\sigma(\\xi) - \\frac{1}{2}] = -\\frac{1}{4}\\frac{1 - e^{-\\xi}}{\\xi(1 + e^{-\\xi})}$\n","        * Update mean and variance for approximate posterior q(w) using <br>\n","        $\\Sigma_{N}^{-1} = E[\\alpha]I + 2\\Sigma_{n=1}^{N}\\lambda(\\xi_{n}X_{n}X_{n}^{T})$ <br>\n","        $\\Sigma_{N}^{-1}\\mu_{N} = \\Sigma_{n=1}^{N} (t_{n} - \\frac{1}{2})X$ <br>\n","        $E[\\alpha] = \\frac{a_{N}}{b_{N}}$\n","      * Update q(alpha) (a is constant, b needs to updated) <br>\n","        $a_{N} = a_{0} + \\frac{M}{2}$ where M: Number of Features (Fixed)<br>\n","        $b_{N} = b_{0} + \\frac{1}{2}E[w^{T}w]$ <br>\n","        $E[w^{T}w] = \\Sigma_{N} + \\mu_{N}^{T}\\mu_{N}$ <br>\n","    * Step 5.2.2: M-Step: Update Parameters eps which controls accuracy of local variational approximation to lower bound $(\\xi_{n}^{new})^{2} = X_{n}^{T}(\\Sigma_{N} + \\mu_{N}\\mu_{N}^{T})X_{n}$\n","* Step 6: Test using Testing Dataset\n","  * Step 6.1: Use probit function equation as genesis equation:\n","    * $\\sigma(\\frac{\\mu.X + bias}{\\sqrt{1 + \\pi\\Sigma^{2}/8}})$ \n","  * Step 6.2: Calculate Accuracy using Sklearns.Metrics library."]},{"cell_type":"code","metadata":{"id":"N_t2983ZX7t7","executionInfo":{"status":"ok","timestamp":1603743828729,"user_tz":240,"elapsed":587,"user":{"displayName":"Mihir Hemant Chauhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilYoSLpXB8B3bEittwsBFYECo7WO6QjgO2KEdR=s64","userId":"14043489589883647341"}},"outputId":"54128e76-05ab-44bc-9959-06854eda98d5","colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["from scipy.special import expit, exprel\n","from scipy.linalg import solve_triangular\n","\n","class BayesianLogisticRegression():\n","\n","    def __init__(self, n_iter, tol):\n","        self.n_iter = n_iter\n","        self.tol = tol\n","\n","    def fit(self, X, y):\n","        '''\n","        Fits Bayesian Logistic Regression\n","        Parameters\n","        -----------\n","        X: array-like of size (n_samples, n_features)\n","           Training data, matrix of explanatory variables\n","\n","        y: array-like of size (n_samples, )\n","           Target values\n","\n","        Returns\n","        -------\n","        self: object\n","           self\n","        '''\n","\n","        # Get the number of classes\n","        self.classes_ = np.unique(y)\n","        n_classes = len(self.classes_)\n","\n","        # Augment X with ones to include bias\n","        X = self._add_intercept(X)\n","\n","        self.coef_, self.sigma_, self.intercept_ = [0], [0], [0]\n","\n","        # make classifier for just binary classification\n","        coef_, sigma_ = self._fit(X, y) ## Calling VBLR Fit\n","        self.intercept_[0], self.coef_[0] = self._get_intercept(coef_)\n","        self.sigma_[0] = sigma_\n","        self.coef_ = np.asarray(self.coef_)\n","        return self\n","\n","    def predict_prob(self, X):\n","        print(\"Testing Logistic Regression Model (Calculating Class Probabilities) . .\")\n","        '''\n","        Predicts probabilities of targets for test set\n","\n","        Parameters\n","        ----------\n","        X: array-like of size [n_samples_test,n_features]\n","           Matrix of explanatory variables (test set)\n","\n","        Returns\n","        -------\n","        probs: numpy array of size [n_samples_test]\n","           Estimated probabilities of target classes\n","        '''\n","        # construct separating hyperplane\n","        scores = (np.dot(X,self.coef_.T) + self.intercept_).flatten()\n","        X = self._add_intercept(X)\n","\n","        # probit approximation to predictive distribution\n","        sigma = self._get_sigma(X)\n","        ks = 1. / (1. + np.pi * sigma / 8) ** 0.5\n","        probs = expit(scores.T * ks).T\n","\n","        return probs\n","\n","# ============== VB Logistic Regression (with Jaakola Jordan bound) ==================s\n","\n","def lam(eps):\n","    ''' Calculates lambda eps (used for Jaakola & Jordan local bound) '''\n","    eps = -abs(eps)\n","    return 0.25 * exprel(eps) / (np.exp(eps) + 1)\n","\n","class VBLogisticRegression(BayesianLogisticRegression):\n","    '''\n","    Variational Bayesian Logistic Regression with local variational approximation.\n","\n","\n","    Parameters:\n","    -----------\n","    n_iter: int, optional (DEFAULT = 50 )\n","       Maximum number of iterations\n","\n","    tol: float, optional (DEFAULT = 1e-3)\n","       Convergence threshold, if cange in coefficients is less than threshold\n","       algorithm is terminated\n","\n","    fit_intercept: bool, optinal ( DEFAULT = True )\n","       If True uses bias term in model fitting\n","\n","    a: float, optional (DEFAULT = 1e-6)\n","       Rate parameter for Gamma prior on precision parameter of coefficients\n","\n","    b: float, optional (DEFAULT = 1e-6)\n","       Shape parameter for Gamma prior on precision parameter of coefficients\n","\n","\n","    Attributes\n","    ----------\n","    coef_ : array, shape = (n_features)\n","        Coefficients of the regression model (mean of posterior distribution)\n","    sigma_ : array, shape = (n_features, n_features)\n","        estimated covariance matrix of the weights, computed only\n","        for non-zero coefficients\n","    intercept_: array, shape = (n_features)\n","        intercepts\n","\n","    References:\n","    -----------\n","   [1] Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )\n","   [2] Murphy 2012, Machine Learning A Probabilistic Perspective ( Chapter 21 )\n","    '''\n","\n","    def __init__(self, n_iter=50, tol=1e-3,\n","                 a=1e-4, b=1e-4):\n","        super(VBLogisticRegression, self).__init__(n_iter, tol)\n","        self.a = a\n","        self.b = b\n","        # self._mask_val = 0.\n","\n","    def _fit(self, X, y):\n","        '''\n","        Fits single classifier\n","        '''\n","        print(\"Training Logistic Regression Model with Variational Bayes EM approach . .\")\n","        # Initialize Variables for Training (eps, bias (w0), parameters of approaximate distribution (a,b))\n","        eps = 1\n","        n_samples, n_features = X.shape\n","        XY = np.dot(X.T, (y - 0.5))\n","        w0 = np.zeros(n_features)\n","\n","        # hyperparameters of q(alpha) (approximate distribution of precision\n","        # parameter of weights)\n","        a = self.a + 0.5 * n_features\n","        b = self.b\n","\n","        for i in range(self.n_iter):\n","            # In the E-step we update approximation of\n","            # posterior distribution q(w,alpha) = q(w)*q(alpha)\n","            # --------- update q(w) ------------------\n","            l = lam(eps)\n","            w, Ri = self._posterior_dist(X, l, a, b, XY)\n","            # -------- update q(alpha) ---------------\n","            b = self.b + 0.5 * (np.sum(w[1:] ** 2) + np.sum(Ri[1:, :] ** 2))\n","\n","            # -------- update eps  ------------\n","            # In the M-step we update parameter eps which controls\n","            # accuracy of local variational approximation to lower bound\n","            XMX = np.dot(X, w) ** 2\n","            XSX = np.sum(np.dot(X, Ri.T) ** 2, axis=1)\n","            eps = np.sqrt(XMX + XSX)\n","\n","            # convergence\n","            if np.sum(abs(w - w0) > self.tol) == 0 or i == self.n_iter - 1:\n","                break\n","            w0 = w\n","\n","        l = lam(eps)\n","        coef_, sigma_ = self._posterior_dist(X, l, a, b, XY, True)\n","        return coef_, sigma_\n","\n","    def _add_intercept(self, X):\n","        '''Adds intercept to data matrix'''\n","        return np.hstack((np.ones([X.shape[0], 1]), X))\n","\n","    def _get_intercept(self, coef):\n","        ''' Returns intercept and coefficients '''\n","        return coef[0], coef[1:]\n","\n","    def _get_sigma(self, X):\n","        ''' Compute variance of predictive distribution'''\n","        return np.asarray([np.sum(np.dot(X, s) * X, axis=1) for s in self.sigma_])\n","\n","    def _posterior_dist(self, X, l, a, b, XY, full_covar=False):\n","        '''\n","        Finds gaussian approximation to posterior of coefficients using\n","        local variational approximation of Jaakola & Jordan\n","        '''\n","        sigma_inv = 2 * np.dot(X.T * l, X)\n","        alpha_vec = np.ones(X.shape[1]) * float(a) / b\n","        alpha_vec[0] = np.finfo(np.float16).eps # Bias variable\n","        np.fill_diagonal(sigma_inv, np.diag(sigma_inv) + alpha_vec) # sigma inv\n","        R = np.linalg.cholesky(sigma_inv)\n","        Z = solve_triangular(R, XY, lower=True)\n","        mean = solve_triangular(R.T, Z, lower=False)\n","\n","        Ri = solve_triangular(R, np.eye(X.shape[1]), lower=True)\n","        if full_covar:\n","            sigma = np.dot(Ri.T, Ri)\n","            return mean, sigma\n","        else:\n","            return mean, Ri\n","\n","# Step 1 already implemented\n","import pandas as pd\n","import io\n","import requests\n","\n","url = \"https://raw.githubusercontent.com/Mihir2/BreakoutSessionDataset/master/titanic_lr.csv\"\n","s = requests.get(url).content\n","data = pd.read_csv(io.StringIO(s.decode('utf-8')))\n","\n","# Step 2 already implemented\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","output = data['y']\n","input = data.to_numpy()[:, 1:]\n","x_train, x_test, y_train, y_test = train_test_split(input, output, test_size=0.2)\n","\n","# Step 3 already implemented\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","x_train_arr = scaler.fit_transform(x_train)\n","x_test_arr = scaler.transform(x_test)\n","\n","# Step 5 already implemented\n","y_train_arr = y_train.to_numpy()\n","y_test_arr = y_test.to_numpy()\n","\n","# Step 6 Train using Variational Bayesian Regression Class (Calculate model parameters)\n","VBLR = VBLogisticRegression()\n","VBLR.fit(x_train_arr,y_train_arr)\n","\n","\n","# Step 7 Test using Variational Bayesian Regression Class (Calculate class Probabilities)\n","y_preds = VBLR.predict_prob(x_test_arr)\n","y_preds[y_preds>=0.5]=1\n","y_preds[y_preds<0.5]=0\n","\n","#6.2\n","# Step 8 Calculate Accuracy using Sklearns library\n","from sklearn.metrics import accuracy_score\n","accuracy_score(y_test_arr.flatten(),y_preds.flatten())"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Training Logistic Regression Model with Variational Bayes EM approach . .\n","Testing Logistic Regression Model (Calculating Class Probabilities) . .\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.7132867132867133"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"JZUNQOPcfC2o","executionInfo":{"status":"ok","timestamp":1603744093573,"user_tz":240,"elapsed":686,"user":{"displayName":"Mihir Hemant Chauhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilYoSLpXB8B3bEittwsBFYECo7WO6QjgO2KEdR=s64","userId":"14043489589883647341"}},"outputId":"b32969ff-5424-4a68-c80e-0e8226615ae3","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import pandas as pd\n","import io\n","import requests\n","url = \"https://raw.githubusercontent.com/Mihir2/BreakoutSessionDataset/master/data_banknote_authentication_lr.csv\"\n","s = requests.get(url).content\n","data = pd.read_csv(io.StringIO(s.decode('utf-8')))\n","\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","output = data['y']\n","input = data.to_numpy()[:, 1:]\n","x_train, x_test, y_train, y_test = train_test_split(input, output, test_size=0.2)\n","\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","x_train_arr = scaler.fit_transform(x_train)\n","x_test_arr = scaler.transform(x_test)\n","y_train_arr = y_train.to_numpy()\n","y_test_arr = y_test.to_numpy()\n","\n","tol = 0.001\n","a = 0.0001\n","eps = 1\n","n_samples, n_features = x_train_arr.shape\n","w0 = np.zeros(n_features + 1)\n","coef, sigma, intercept = [0], [0], [0]\n","n_iter = 100\n","a = a + 0.5 * n_features\n","b = 0.0001\n","x_train_arr = np.hstack((np.ones([x_train_arr.shape[0], 1]), x_train_arr))\n","\n","from scipy.special import expit, exprel\n","from scipy.linalg import solve_triangular\n","for i in range(n_iter):\n","    eps = -abs(eps)\n","    l = 0.25 * exprel(eps) / (np.exp(eps) + 1)\n","\n","    sigma_inv = 2 * np.dot(x_train_arr  .T * l, x_train_arr)\n","    alpha_vec = np.ones(x_train_arr.shape[1]) * float(a) / b\n","    alpha_vec[0] = np.finfo(np.float16).eps\n","    np.fill_diagonal(sigma_inv, np.diag(sigma_inv) + alpha_vec)\n","    R = np.linalg.cholesky(sigma_inv)\n","    Z = solve_triangular(R, np.dot(x_train_arr.T, (y_train_arr- 0.5)), lower=True)\n","    mean_ = solve_triangular(R.T, Z, lower=False)\n","    sigma_ = solve_triangular(R, np.eye(x_train_arr.shape[1]), lower=True)\n","\n","    b = b + 0.5 * (np.sum(mean_[1:] ** 2) + np.sum(sigma_[1:, :] ** 2))\n","\n","    XMX = np.dot(x_train_arr, mean_) ** 2\n","    XSX = np.sum(np.dot(x_train_arr, sigma_.T) ** 2, axis=1)\n","    eps = np.sqrt(XMX + XSX)\n","\n","    if np.sum(abs(mean_ - w0) > tol) == 0 or i == n_iter - 1:\n","      break\n","    w0 = mean_\n","\n","intercept[0] = mean_[0]\n","sigma[0] = sigma_\n","mean = mean_[1:]\n","scores = (np.dot(x_test_arr,mean.T) + intercept).flatten()\n","x_test_arr = np.hstack((np.ones([x_test_arr.shape[0], 1]), x_test_arr))\n","si = np.asarray([np.sum(np.dot(x_test_arr, s) *x_test_arr, axis=1) for s in sigma])\n","ks = ks = 1. / (1. + np.pi * si / 8) ** 0.5\n","y_pred = expit(scores.T * ks).T\n","y_pred[y_pred>=0.5]=1\n","y_pred[y_pred<0.5]=0\n","\n","from sklearn.metrics import accuracy_score\n","accuracy_score(y_test_arr.flatten(),y_pred.flatten())"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9890909090909091"]},"metadata":{"tags":[]},"execution_count":13}]}]}