{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Room_3_Ganden.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hdNWzJyq94JQ"},"source":["# List Full Names of all the participants in your team below:\n","1. Arghya Dutta\n","2. Daniel Ip\n","3. Gaurav Ravindra Toravane\n","4. Joyce Sommer\n","5. Mengyuan\n","6. Michael Murphy\n","7. Mohan Vellayan\n","8. Sindhu Pvp\n","9. Yuan Meng\n","10. \n","11. "]},{"cell_type":"markdown","metadata":{"id":"ETKENBFN-dMb"},"source":["Hello Machine Learning Engineer Ganden Team, \n","\n","You have been given a **Titanic Survival Dataset**. The sinking of the Titanic is one of the most infamous shipwrecks in history. \n","\n","Number of Instances: 712 <br>\n","Number of Attributes: 8 (including the target variable `y`)\n","\n","Attribute Information: \n","\n","* **y**  Survival\n","    * 0 = No\n","    * 1 = Yes\n","* **f1** Passenger class\n","    * 1 = 1st class\n","    * 2 = 2nd class\n","    * 3 = 3rd class\n","* **f2** Sex\n","    * 0 = Male\n","    * 1 = Female\n","* **f3** Age in years\n","* **f4** # of siblings / spouses aboard the Titanic\n","* **f5** # of parents / children aboard the Titanic\n","* **f6** Passenger Fare\n","* **f7** Port of Embarkation\n","    * 0 = Southampton\n","    * 1 = Cherbourg\n","    * 2 = Queenstown\n","\n","There are no missing Attribute Values.\n","\n","Your task is to implement a **Logistic Regression model with Variational Bayesian EM Aprroach** to predict if the passenger on Titanic Survived or not."]},{"cell_type":"markdown","metadata":{"id":"i4KEiQSx-7Ao"},"source":["\n","## Variational Bayesian Logistic Regression\n","\n","Similar to the Laplace Method for Bayesian Logistic Regression, the Variational Methods focuses on the use of Gaussian Approximation to the posterior distribution. In fact the variational approcimation to the posterior distribution leads to improved accuracy compared to the laplace method. The variational approach is optimizing a well defined objective function given by a rigorous bound on the model evidence. \n","\n","\n","\n","<font color=\"Green\">USE THE VBLogisticRegression CLASS DIRECTLY.</font>"]},{"cell_type":"code","metadata":{"id":"mGONrvsaW_Af"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDjGo3Fu-c5l"},"source":["### **Question 1:** In the following code cell implement Step 5, 6 and 7. \n","### **Question 2:** MAP the formulaes to the code snippets of VBLR class in Step 5.1, Step 5.2 and Step 6.1 and Step 6.2 : \n","* Step 1: Import the dataset using Pandas Dataframe (Step 1 Implemented already)\n","* Step 2: Partition your dataset into training and testing using sklearns train_test_split library and split the features and target labels into seperate variables (Step 2 Implemented already)\n","* Step 3: Scale the training and testing features using sklearns min max scaling function (Step 3 Implemented already)\n","* Step 4: Convert Labels into numpy arrays (Step 4 Implemented already)\n","* Step 5: Train with Training Dataset using VBLR Solution. Below are the steps required for training VBLR model:\n","  * Step 5.1: Initialize Variables for Training (eps, bias (w0), parameters of approximate distribution (a,b)) \n","  * Step 5.2: Run EM algorithm Iteratively to update approximate variational posterior distribution q(w, alpha): \n","    * Step 5.2.1: E-Step: Update approximation of posterior distribution q(w, alpha) = q(w)*q(alpha) \n","      * Update q(w) \n","        * Find lambda(eps) using $\\lambda (\\xi) = -\\frac{1}{2\\xi}[\\sigma(\\xi) - \\frac{1}{2}] = -\\frac{1}{4}\\frac{1 - e^{-\\xi}}{\\xi(1 + e^{-\\xi})}$\n","        * Update mean and variance for approximate posterior q(w) using <br>\n","        $\\Sigma_{N}^{-1} = E[\\alpha]I + 2\\Sigma_{n=1}^{N}\\lambda(\\xi_{n}X_{n}X_{n}^{T})$ <br>\n","        $\\Sigma_{N}^{-1}\\mu_{N} = \\Sigma_{n=1}^{N} (t_{n} - \\frac{1}{2})X$ <br>\n","        $E[\\alpha] = \\frac{a_{N}}{b_{N}}$\n","      * Update q(alpha) (a is constant, b needs to updated) <br>\n","        $a_{N} = a_{0} + \\frac{M}{2}$ where M: Number of Features (Fixed)<br>\n","        $b_{N} = b_{0} + \\frac{1}{2}E[w^{T}w]$ <br>\n","        $E[w^{T}w] = \\Sigma_{N} + \\mu_{N}^{T}\\mu_{N}$ <br>\n","    * Step 5.2.2: M-Step: Update Parameters eps which controls accuracy of local variational approximation to lower bound $(\\xi_{n}^{new})^{2} = X_{n}^{T}(\\Sigma_{N} + \\mu_{N}\\mu_{N}^{T})X_{n}$\n","* Step 6: Test using Testing Dataset\n","  * Step 6.1: Use probit function equation as genesis equation:\n","    * $\\sigma(\\frac{\\mu.X + bias}{\\sqrt{1 + \\pi\\Sigma^{2}/8}})$ \n","  * Step 6.2: Calculate Accuracy using Sklearns.Metrics library."]},{"cell_type":"code","metadata":{"id":"Z0u1PqcwYftd","executionInfo":{"status":"ok","timestamp":1603320638811,"user_tz":240,"elapsed":533,"user":{"displayName":"Gaurav Ravindra Toravane","photoUrl":"","userId":"03834369426800348659"}},"outputId":"550854e2-65b8-4375-f93a-c6217b13304f","colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["from scipy.special import expit, exprel\n","from scipy.linalg import solve_triangular\n","\n","class BayesianLogisticRegression():\n","\n","    def __init__(self, n_iter, tol):\n","        self.n_iter = n_iter\n","        self.tol = tol\n","\n","    def fit(self, X, y):\n","        '''\n","        Fits Bayesian Logistic Regression\n","        Parameters\n","        -----------\n","        X: array-like of size (n_samples, n_features)\n","           Training data, matrix of explanatory variables\n","\n","        y: array-like of size (n_samples, )\n","           Target values\n","\n","        Returns\n","        -------\n","        self: object\n","           self\n","        '''\n","\n","        # Get the number of classes\n","        self.classes_ = np.unique(y)\n","        n_classes = len(self.classes_)\n","\n","        # Augment X with ones to include bias\n","        X = self._add_intercept(X)\n","\n","        self.coef_, self.sigma_, self.intercept_ = [0], [0], [0]\n","\n","        # make classifier for just binary classification\n","        coef_, sigma_ = self._fit(X, y) ## Calling VBLR Fit\n","        self.intercept_[0], self.coef_[0] = self._get_intercept(coef_)\n","        self.sigma_[0] = sigma_\n","        self.coef_ = np.asarray(self.coef_)\n","        return self\n","\n","    def predict_prob(self, X):\n","        print(\"Testing Logistic Regression Model (Calculating Class Probabilities) . .\")\n","        '''\n","        Predicts probabilities of targets for test set\n","\n","        Parameters\n","        ----------\n","        X: array-like of size [n_samples_test,n_features]\n","           Matrix of explanatory variables (test set)\n","\n","        Returns\n","        -------\n","        probs: numpy array of size [n_samples_test]\n","           Estimated probabilities of target classes\n","        '''\n","        # construct separating hyperplane\n","        scores = (np.dot(X,self.coef_.T) + self.intercept_).flatten()\n","        X = self._add_intercept(X)\n","\n","        #6.1\n","        # probit approximation to predictive distribution\n","        sigma = self._get_sigma(X)\n","        ks = 1. / (1. + np.pi * sigma / 8) ** 0.5\n","        probs = expit(scores.T * ks).T\n","\n","        return probs\n","\n","# ============== VB Logistic Regression (with Jaakola Jordan bound) ==================s\n","\n","def lam(eps):\n","    ''' Calculates lambda eps (used for Jaakola & Jordan local bound) '''\n","    eps = -abs(eps)\n","    return 0.25 * exprel(eps) / (np.exp(eps) + 1)\n","\n","class VBLogisticRegression(BayesianLogisticRegression):\n","    '''\n","    Variational Bayesian Logistic Regression with local variational approximation.\n","\n","\n","    Parameters:\n","    -----------\n","    n_iter: int, optional (DEFAULT = 50 )\n","       Maximum number of iterations\n","\n","    tol: float, optional (DEFAULT = 1e-3)\n","       Convergence threshold, if cange in coefficients is less than threshold\n","       algorithm is terminated\n","\n","    fit_intercept: bool, optinal ( DEFAULT = True )\n","       If True uses bias term in model fitting\n","\n","    a: float, optional (DEFAULT = 1e-6)\n","       Rate parameter for Gamma prior on precision parameter of coefficients\n","\n","    b: float, optional (DEFAULT = 1e-6)\n","       Shape parameter for Gamma prior on precision parameter of coefficients\n","\n","\n","    Attributes\n","    ----------\n","    coef_ : array, shape = (n_features)\n","        Coefficients of the regression model (mean of posterior distribution)\n","    sigma_ : array, shape = (n_features, n_features)\n","        estimated covariance matrix of the weights, computed only\n","        for non-zero coefficients\n","    intercept_: array, shape = (n_features)\n","        intercepts\n","\n","    References:\n","    -----------\n","   [1] Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )\n","   [2] Murphy 2012, Machine Learning A Probabilistic Perspective ( Chapter 21 )\n","    '''\n","\n","    def __init__(self, n_iter=50, tol=1e-3,\n","                 a=1e-4, b=1e-4):\n","        super(VBLogisticRegression, self).__init__(n_iter, tol)\n","        self.a = a\n","        self.b = b\n","        self._mask_val = 0.\n","\n","    def _fit(self, X, y):\n","        '''\n","        Fits single classifier for each class (for OVR framework)\n","        '''\n","        print(\"Training Logistic Regression Model with Variational Bayes EM approach . .\")\n","        eps = 1\n","        n_samples, n_features = X.shape\n","        XY = np.dot(X.T, (y - 0.5))\n","        w0 = np.zeros(n_features)\n","\n","        # hyperparameters of q(alpha) (approximate distribution of precision\n","        # parameter of weights)\n","        a = self.a + 0.5 * n_features\n","        b = self.b\n","\n","        for i in range(self.n_iter):\n","            #5.2.1\n","            # In the E-step we update approximation of\n","            # posterior distribution q(w,alpha) = q(w)*q(alpha)\n","            # --------- update q(w) ------------------\n","            l = lam(eps)\n","            w, Ri = self._posterior_dist(X, l, a, b, XY)\n","            # -------- update q(alpha) ---------------\n","            b = self.b + 0.5 * (np.sum(w[1:] ** 2) + np.sum(Ri[1:, :] ** 2))\n","\n","            #5.2.2\n","            # -------- update eps  ------------\n","            # In the M-step we update parameter eps which controls\n","            # accuracy of local variational approximation to lower bound\n","            XMX = np.dot(X, w) ** 2\n","            XSX = np.sum(np.dot(X, Ri.T) ** 2, axis=1)\n","            eps = np.sqrt(XMX + XSX)\n","\n","            # convergence\n","            if np.sum(abs(w - w0) > self.tol) == 0 or i == self.n_iter - 1:\n","                break\n","            w0 = w\n","\n","        l = lam(eps)\n","        coef_, sigma_ = self._posterior_dist(X, l, a, b, XY, True)\n","        return coef_, sigma_\n","\n","    def _add_intercept(self, X):\n","        '''Adds intercept to data matrix'''\n","        return np.hstack((np.ones([X.shape[0], 1]), X))\n","\n","    def _get_intercept(self, coef):\n","        ''' Returns intercept and coefficients '''\n","        return coef[0], coef[1:]\n","\n","    def _get_sigma(self, X):\n","        ''' Compute variance of predictive distribution'''\n","        return np.asarray([np.sum(np.dot(X, s) * X, axis=1) for s in self.sigma_])\n","\n","    def _posterior_dist(self, X, l, a, b, XY, full_covar=False):\n","        '''\n","        Finds gaussian approximation to posterior of coefficients using\n","        local variational approximation of Jaakola & Jordan\n","        '''\n","        sigma_inv = 2 * np.dot(X.T * l, X)\n","        alpha_vec = np.ones(X.shape[1]) * float(a) / b\n","        alpha_vec[0] = np.finfo(np.float16).eps # Bias variable\n","        np.fill_diagonal(sigma_inv, np.diag(sigma_inv) + alpha_vec) # sigma inv\n","        R = np.linalg.cholesky(sigma_inv)\n","        Z = solve_triangular(R, XY, lower=True)\n","        mean = solve_triangular(R.T, Z, lower=False)\n","\n","        Ri = solve_triangular(R, np.eye(X.shape[1]), lower=True)\n","        if full_covar:\n","            sigma = np.dot(Ri.T, Ri)\n","            return mean, sigma\n","        else:\n","            return mean, Ri\n","\n","# Step 1 already implemented\n","import pandas as pd\n","import io\n","import requests\n","\n","url = \"https://raw.githubusercontent.com/Mihir2/BreakoutSessionDataset/master/titanic_lr.csv\"\n","s = requests.get(url).content\n","data = pd.read_csv(io.StringIO(s.decode('utf-8')))\n","\n","# Step 2 already implemented\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","output = data['y']\n","input = data.to_numpy()[:, 1:]\n","x_train, x_test, y_train, y_test = train_test_split(input, output, test_size=0.2)\n","\n","# Step 3 already implemented\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","x_train_arr = scaler.fit_transform(x_train)\n","x_test_arr = scaler.transform(x_test)\n","\n","# Step 5 already implemented\n","y_train_arr = y_train.to_numpy()\n","y_test_arr = y_test.to_numpy()\n","\n","# Step 6 Train using Variational Bayesian Regression Class (Calculate model parameters)\n","VBLR = VBLogisticRegression()\n","VBLR.fit(x_train_arr,y_train_arr)\n","\n","\n","# Step 7 Test using Variational Bayesian Regression Class (Calculate class Probabilities)\n","y_preds = VBLR.predict_prob(x_test_arr)\n","y_preds = (y_preds >= 0.5) * 1  \n","\n","#6.2\n","# Step 8 Calculate Accuracy using Sklearns library\n","from sklearn.metrics import accuracy_score\n","accuracy_score(y_test_arr,y_preds)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Logistic Regression Model with Variational Bayes EM approach . .\n","Testing Logistic Regression Model (Calculating Class Probabilities) . .\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.8321678321678322"]},"metadata":{"tags":[]},"execution_count":4}]}]}