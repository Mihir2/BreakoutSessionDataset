{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Room_1_Sera.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"E2Ya83ax3LWL"},"source":["# List Full Names of all the participants in your team below:\n","1. Yousuf Aziz\n","2. Messiah Smith-Bonet\n","3. Surya Muthiah Pillai\n","4. Yuan Meng\n","5. Eric Yang\n","6. David Tan\n","7. Daniel Ip\n","8. Joyce Sommer \n","9. Kurt Su\n","10. Jonathan Romano\n","11. Xianxin Lin\n"]},{"cell_type":"markdown","metadata":{"id":"vuDjM-sd3PRp"},"source":["Hello Machine Learning Engineer Sera Team, \n","\n","You have been given a data which is obtained from **Air Quality** of Seattle City. The dataset contains 788 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device.\n","\n","Number of Instances: 789 <br>\n","Number of Attributes: 14 (including the target variable `y`)\n","\n","Attribute Information: \n","* **y**  AQI Air Quality Index\n","* **f1** True hourly averaged concentration CO in mg/m^3 (reference analyzer)\n","* **f2** PT08.S1 (tin oxide) hourly averaged sensor response (nominally CO targeted)\n","* **f3** True hourly averaged overall Non Metanic HydroCarbons concentration in microg/m^3 (reference analyzer)\n","* **f4** True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)\n","* **f5** PT08.S2 (titania) hourly averaged sensor response (nominally NMHC targeted)\n","* **f6** True hourly averaged NOx concentration in ppb (reference analyzer)\n","* **f7** PT08.S3 (tungsten oxide) hourly averaged sensor response (nominally NOx targeted)\n","* **f8** True hourly averaged NO2 concentration in microg/m^3 (reference analyzer)\n","* **f9** PT08.S4 (tungsten oxide) hourly averaged sensor response (nominally NO2 targeted)\n","* **f10** PT08.S5 (indium oxide) hourly averaged sensor response (nominally O3 targeted)\n","* **f11** Temperature in Â°C\n","* **f12** Relative Humidity (%)\n","* **f13** AH Absolute Humidity\n","\n","There are no missing Attribute Values.\n","\n","Your task is to implement a **Gaussian Radial Basis Function based Linear Regression model using Closed Form Solution** for predicting the Air Quality Index for Seattle City.\n","\n","## Closed Form Solution with Basis Functions\n","The **genesis equation** for Linear Regression with Gaussian Basis Function is of the form:\n","\n","$y(x,w) = \\phi(x).W$  \n","\n","* $y(x,w)$ is predicted output,\n","* $\\phi(x)$ is the Design Matrix\n","* $W = (w_{1}, ... w_{M})$ are the parameters to be learned from training samples\n","\n","### Design Matrix\n","Each gaussian radial basis function $\\phi_{j}$ converts the input instance to a value as shown below: <br>\n","\n","$\\phi_{j}(x) = \\exp(-\\frac{1}{2}(x - \\mu_{j})^{T}\\sum_{j}^{-1}(x - \\mu_{j}))$\n","\n","* $x$ is the input scaled dataset <br>\n","* $\\mu_{j}$ is the center of the $j_{th}$ Guassian Radial Basis Function <br>\n","* $\\sum_{j}$ decides how braodly the $j_{th}$ basis function spreads (Diagonal Covariance Matrix)\n","\n","Repeated application of $j$ basis functions results in a Design Matrix as shown below:\n","![!picture](https://drive.google.com/uc?export=view&id=1j1kxv6nUPPECacd-_bDg_lL1yTJS5BwA)\n","\n","For finding parameters $W$ for the above genesis using the **closed form solution** we pre-multiply by $\\phi^{-1}(x)$ on LHS and RHS. We get,\n","\n","$W = \\phi^{-1}(x)Y$\n","\n","But $\\phi(x)$ is NOT A SQUARE MATRIX of FULL RANK! Hence, $\\phi^{-1}(x)$ is intractable.\n","\n","We therefore use the Moore-Penrose pseudo inverse as a generalization of the matrix inverse when the matrix may not be invertible. Hence, the final closed form solution for finding parameters $W$ with linear regression least squares solution is as follows:\n","\n","$W = (\\phi^{T}\\phi)^{-1}\\phi^{T}Y$\n","\n","YOU NEED TO IMPLEMENT ABOVE EQUATION for finding $W$. \n","\n","<font color=\"red\"> YOU CANNOT USE NUMPY linalg **pinv** https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html </font>\n","\n","<font color=\"red\">DO NOT USE SKLEARNS LINEAR REGRESSION LIBRARY DIRECTLY.</font>\n","\n","<font color=\"green\">YOU CAN USE np.linalg.inv, and np.dot FOR IMPLEMENTING PSEUDO-INVERSE</font>"]},{"cell_type":"markdown","metadata":{"id":"ffzBTXt_3TDj"},"source":["### **Question:** In the following code cell implement the following:\n","* Step 1: Import the dataset (AirQualitySeattle.csv) using Pandas Dataframe (Step 1 Implemented already)\n","* Step 2: Partition your dataset into training testing and validation using sklearns train_test_split library and split the features and target labels into seperate variables (Step 2 Implemented already)\n","* Step 3: Scale the features using sklearns min max scaling function (Step 3 Implemented already)\n","* Step 4: Convert Scaled Features and Labels into numpy arrays with dimensions required by closed form solution (Step 4 Implemented already)\n","* Step 5: Find the Mean ($\\mu_{j}$) and Spread ($\\sum_{j}$) for **3 basis functions** (Step 5 Implemented Already)\n","* Step 6: Create a Design Matrix using the scaled features, Mean ($\\mu_{j}$) and Spread ($\\sum_{j}$)\n","* Step 7: Train using Linear Regression algorithm with a Closed Form Solution **Hint: Use Pseudo Inverse Formula**\n","* Step 8: Test using Testing Dataset (Make sure you create a design matrix for Testing dataset using same Mean ($\\mu_{j}$) and Spread ($\\sum_{j}$) from Step 5)\n","* Step 9: Calculate Root Mean Squared Error (Erms) for Test Dataset\n","    * $Erms = \\sqrt{\\frac{1}{n}\\sum_{i=0}^{i=n} (y\\_test_{i} - y\\_test\\_pred_{i})^{2}}$ "]},{"cell_type":"code","metadata":{"id":"QPS0seHc23xM","executionInfo":{"status":"ok","timestamp":1602700141936,"user_tz":240,"elapsed":832,"user":{"displayName":"Mihir Hemant Chauhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilYoSLpXB8B3bEittwsBFYECo7WO6QjgO2KEdR=s64","userId":"14043489589883647341"}}},"source":["# Step 1 already implemented\n","import pandas as pd\n","import io\n","import requests\n","url=\"https://raw.githubusercontent.com/Mihir2/BreakoutSessionDataset/master/AirQualitySeattle.csv\"\n","s = requests.get(url).content\n","data = pd.read_csv(io.StringIO(s.decode('utf-8')))\n","data\n","\n","# Step 2 already implemented\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","output = data['y']\n","input = data.to_numpy()[:,1:]\n","x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.2)\n","\n","# Step 3 already implemented\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","sc_xtrain = scaler.fit_transform(x_train)\n","sc_xtest = scaler.transform(x_test)\n","\n","# Step 4 already implemented\n","y_train_arr = y_train.to_numpy().reshape(y_train.shape[0],1)\n","x_train_arr = sc_xtrain\n","y_test_arr  = y_test.to_numpy().reshape(y_test.shape[0],1)\n","x_test_arr  = sc_xtest\n","\n","# Step 5 already implemented\n","from  sklearn.cluster import MiniBatchKMeans\n","number_of_basis_function = 3\n","model = MiniBatchKMeans(n_clusters=number_of_basis_function)\n","distances = model.fit_transform(x_train_arr)\n","basis_means = model.cluster_centers_\n","basis_variances = np.zeros(number_of_basis_function)\n","i = 0\n","for label in model.labels_:\n","  basis_variances[label] = basis_variances[label] + (distances[i][label]**2)\n","  i = i + 1\n","for j in range(0,number_of_basis_function):\n","  basis_variances[j] = basis_variances[j]/np.count_nonzero(model.labels_ == j)\n","basis_variances = np.diag(basis_variances)\n","\n","#print(basis_means)\n","# print(basis_variances)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bzhC1oVt-qvE"},"source":["## TA Answer"]},{"cell_type":"code","metadata":{"id":"w6W8Bg5M1-WI","executionInfo":{"status":"ok","timestamp":1600978881319,"user_tz":240,"elapsed":1565,"user":{"displayName":"Mihir Hemant Chauhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilYoSLpXB8B3bEittwsBFYECo7WO6QjgO2KEdR=s64","userId":"14043489589883647341"}},"outputId":"d8a3b49c-a27b-4e8d-c8c1-90cb74c61917","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Step 6\n","x_mu = np.zeros((number_of_basis_function,x_train_arr.shape[0]))\n","for i in range(0,number_of_basis_function):\n","  x_mu[i] = np.sum((x_train_arr - basis_means[i]),axis=1)\n","\n","train_design_mat = np.exp(-0.5*np.multiply(np.dot(x_mu.T,np.linalg.inv(basis_variances)),x_mu.T))\n","\n","# Step 7 \n","weights = np.dot(np.dot(np.linalg.inv(np.dot(train_design_mat.T,train_design_mat)),train_design_mat.T),y_train_arr)\n","\n","# Step 8\n","x_mu = np.zeros((number_of_basis_function,x_test_arr.shape[0]))\n","for i in range(0,number_of_basis_function):\n","  x_mu[i] = np.sum((x_test_arr - basis_means[i]),axis=1)\n","\n","test_design_mat = np.exp(-0.5*np.multiply(np.dot(x_mu.T,np.linalg.inv(basis_variances)),x_mu.T))\n","y_test_pred = np.dot(test_design_mat, weights)\n","\n","#Step 9\n","Erms = np.sqrt(np.sum((y_test_pred - y_test_arr)**2)/y_test_arr.shape[0])\n","print(Erms)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["163.92466954866845\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_ZfDJkgM-tob"},"source":["## Student Response"]},{"cell_type":"code","metadata":{"id":"M5puZQYF1T3B"},"source":["# Step 6 \n","import math as m\n","\n","# newx = model.fit_transform()\n","# mu = model.fit_transform()\n","\n","# x_mu = distances - basis_means\n","\n","basis_transpose = np.transpose(distances)\n","\n","basisByTwo = basis_transpose * -0.5\n","sigma_inv = np.linalg.inv(basis_variances)\n","designmat = np.dot(sigma_inv, distances.T)*distances.T\n","\n","#designmat = m.exp(ibd_bytwo)\n","print(designmat.shape)\n","\n","# Step 7 \n","W=np.dot(np.dot(np.linalg.inv(np.dot(designmat.T,designmat)),designmat.T),y_train_arr)\n","\n","# Step 8 \n","\n","\n","# Step 9 "],"execution_count":null,"outputs":[]}]}